---
title: "p8105_hw3_dl3157"
author: "Ditian Li"
date: "2018/10/11"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
library(tidyverse)
library(ggridges)
library(hexbin)
devtools::install_github("thomasp85/patchwork")
theme_set(theme_bw())
```

## Problem 1
```{r p1}
devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)
brfss_sm = brfss_smart2010 %>%
janitor::clean_names() %>%
filter(topic == 'Overall Health') %>%
 mutate(response = as.factor(response),
         response = factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))) %>% 
  select(year, state = locationabbr, county = locationdesc, response, everything()) 
levels(brfss_sm$response)

```

Comment: We've done data cleaning part and reordered data by response.

Question 1
```{r p1q1}
brfss_sm %>%
  group_by(year,state) %>% 
  summarize(n = n_distinct(county)) %>% 
  filter(n == 7) %>% 
  filter(year == 2002)
  
```

Comment: From result we know, in 2002, CT, FL and NC were observed at 7 locations.

Question 2
```{r p1q2}
brfss_sm %>% 
  group_by(year, state) %>% 
  summarize(n = n_distinct(county)) %>% 
  ggplot(aes(x = year, y = n, color = state)) + 
  geom_point() + geom_line(alpha = .7) + 
  labs( title = "locations in each state from 2002 to 2010",
    x = "Year",
    y = "Number of locations") + 
  theme(legend.position = "left")
```

Comment: From result, from 2002 to 2010 for most states, the number of locations in each state is below 20, however, Florida has two peak value above 40.

Question 3
```{r p1q3}
brfss_sm1 <- brfss_sm %>% 
  spread(key = "response", value = "data_value") %>% 
  janitor::clean_names() %>% 
  select(year, state, county, excellent:poor) %>% 
  filter( year == 2002 | year == 2006 | year == 2010) %>% 
  group_by(county) %>% 
  filter(state == "NY") %>% 
  summarize(mean_excellent = mean(excellent, na.rm = TRUE),
            sd_excellent = sd(excellent, na.rm = TRUE)) %>% 
  knitr::kable(digits = 1)

brfss_sm1

```

Comment: For New York state, the min of excellent mean is 17.2, the max of excellent mean is 27.5, there are three "NA" for sd_excellent, and range for sd_excellent is from 0.6 to 3.3.

Question 4
```{r p1q4}
brfss_sm %>% 
  spread(key = "response", value = "data_value") %>% 
  janitor::clean_names() %>% 
  select(year, state, county, excellent:poor) %>% 
  group_by(year, state) %>% 
  summarize(mean_excellent = mean(excellent, na.rm = TRUE),
            mean_very_good = mean(very_good, na.rm = TRUE),
            mean_good = mean(good, na.rm = TRUE),
            mean_fair = mean(fair, na.rm = TRUE),
            mean_poor = mean(poor, na.rm = TRUE)
            ) %>% 
  gather(key = "mean_response", value = "average_proportion",    
  mean_excellent:mean_poor) %>%
  mutate(mean_response = as.factor(mean_response),
  mean_response = factor(mean_response, levels = c("mean_excellent", "mean_very_good", "mean_good", "mean_fair", "mean_poor"))) %>% 
  ggplot(aes(x = year, y = average_proportion, color = state)) + 
  geom_point() + geom_line() +
  facet_grid(~mean_response) +
  labs(
    title = "Average proportion in each response for each year and each    
    state",
    x = "Year",
    y = "Average proportion"
  ) + theme(legend.position = "none",axis.text.x = element_text(angle = 45))
```

Comment: We have 5 spaghetti plot for mean proportions of different responses. From result it seems that we have same trend and range of value for each response type from 2002-2010 across year.

```{r p2}
devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)
instacart
sum(is.na(instacart))
nrow(distinct(instacart, order_id))
nrow(distinct(instacart, user_id))
nrow(distinct(instacart, product_id))
```

Overview: The dimension of dataset instacart is 1384617*15=20769255, there is no missing data in instacart. For key variables we are interested in, there are 131209 orders, 131209 users and 39123 products in instacart.

Question 1
```{r p2q1}
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_item = n()) %>% 
  arrange(desc(n_item))
```

There are 134 aisles in tha instacart dataset, the most item ordered from fresh  is vegetables, and then it comes to fresh fruits, packaged vegetables fruits(which indicates the great demand in healthy food). 

Question 2
```{r p2q2, fig.width=10, fig.height=20}
instacart1 <- instacart %>% 
  group_by(aisle) %>% 
  summarize(n_item = n()) %>% 
  mutate(group = as.numeric(cut_number(n_item, 3))) %>% 
  ggplot(aes(x = reorder(aisle, n_item), y = n_item)) +
    geom_point() +
    facet_wrap(group ~ ., nrow = 3, scales = "free") +
    theme(axis.text.x = element_text(size = 6, hjust = 1, angle = 45)) +
    labs(
      title = "Number of Items Ordered in Aisles",
      x = "Aisle Name",
      y = "Number of Items Ordered"
      )
instacart1
```

Comment: In this question, we dividies our plot into 3 parts to make it clear. From the result, fresh vegetables and fresh fruits have largest numbers of items ordered in aisles, which is consistent with our conclusion in question above. Beauty has the lowest number of items ordered in aisles. 

Question 3
```{r p2q3}
instacart %>%  
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(count = n()) %>% 
  group_by(aisle) %>% 
  mutate(rank = min_rank(desc(count))) %>%  
  filter(rank < 2)
```

Comment: From result, the most popular item in "baking ingredients","dog food care" and "packaged vegetables fruits" are Light brown sugar, Snack Sticks Chicken & Rice Recipe Dog Treats and Organic Baby Spinach respectively. The order quantity is quite different(Light brown suga=499, Snack Sticks Chicken & Rice Recipe Dog Treats=30, Organic Baby Spinach=9784).

Question 4
```{r p2q4}
instacart %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = round(mean(order_hour_of_day), digits = 1)) %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  spread(key = "product_name", value = "mean_hour") %>% 
  knitr::kable()
```

Comment: order_dow represents Sunday=0 to Saturday=6. People order Pink Lady Apples from 11 to 14 on average, it is the time overlap the lunch break. For 
the Coffee Ice Cream, it is ordered from 13 to 16.

```{r p3}
devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)
ny_noaa

ny_noaa %>% 
  group_by(id) %>% 
  summarize(prcp_na = sum(is.na(prcp)),
            snow_na = sum(is.na(snow)),
            snwd_na = sum(is.na(snwd)),
            tmax_na = sum(is.na(tmax)),
            tmin_na = sum(is.na(tmin)))
```

Description: The dimension of ny_noaa dataset is 2,595,176 x 7, in this dataset we have id, date of observation, precipitation, snowfall in mm, snow depth in mm, maximum temperature and minimum temprature both in degree centigrade on each day across 1981 t0 2010. From result we notice that missing data is a big problem since large proportion of data is missing in ny_noaa.

Question 1
```{r p3q1}
ny_noaa1 <- ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(prcp = prcp / 10,
         tmax = as.numeric(tmax) / 10,
         tmin = as.numeric(tmin) / 10)
  count(ny_noaa, snow) %>% arrange(desc(n))
```

Comment: For snowfall, the most commonly observed value is 0 degree centigrade, because we only have snowfall during winter, which only happens not that frequently in locations we observed.

Question 2
```{r p3q2}
ny_noaa1 %>% 
  group_by(id, year, month) %>% 
  summarize(mean_tmax = round(mean(tmax, na.rm = TRUE), digits = 1),
            mean_tmin = round(mean(tmin, na.rm = TRUE), digits = 1)) %>% 
  na.omit() %>% 
  gather(key = mean_temp, value = temp, mean_tmax) %>%
  filter(month %in% c("01", "07")) %>% 
  ggplot(aes(x = year, y = temp)) + geom_boxplot() +   
  facet_grid(~month) + 
  theme(axis.text.x = element_text(size = 5, angle = 45),legend.position = "bottom") +
   labs(
    title = "Average max temperature in January and July in each station  
    from 1981 to 2010", x = "Year", y = "Average max temperature in ËšC"
  )
```

Comment: From box plot we can conclude that there is approximate same pattern for tmax across years. For tmax in Jan, the range of tmax is from -10 to 5 degree, for tmax in July, the tmax is from 25 to 30. We do have outliers, for example, in Jan 1982, we have an outlier below -10 degree; in July 1987, we have an outlier below 15 degree.

Question 3
```{r p3q3}
ny_noaa2 <- ny_noaa1 %>% 
  ggplot(aes(x = tmax, y = tmin)) + geom_hex()  +
  labs(
    title = "Hex plot of maximum temperature/minimum temperature",
    x = "maximum temperature",
    y = "minimum temperature"
  )
ny_noaa2

ny_noaa3 <- ny_noaa1 %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = snow, y = year)) + geom_density_ridges(scale = 1) +
  labs(
    title = "density plot of snowfall from 0 mm to 100 mm across years",
    x = "snow fall in mm", y = "Year")
ny_noaa3
```

Comment: From hexplot we can conclude that tmax and tmin have approximate linear relationship, the light blue means there are more counts in the plot.From ridges plot of snowfall we know, we have same pattern of snowfall distribution across years.
